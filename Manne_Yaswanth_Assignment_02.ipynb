{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Wednesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed30f304-4f79-40fa-8430-958a0599ac2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1250 reviews have been successfully scraped and saved to 'yash_reviews.csv'.\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def scrape_reviews(url, num_pages, output_file):\n",
        "    reviews_data = []\n",
        "\n",
        "    for page_num in range(1, num_pages + 1):\n",
        "        page_url = f\"{url}&page={page_num}\"\n",
        "\n",
        "        response = requests.get(page_url)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to retrieve page {page_num}. Exiting.\")\n",
        "            break\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        review_containers = soup.find_all('div', class_='lister-item-content')\n",
        "\n",
        "        for container in review_containers:\n",
        "            review_text = container.find('div', class_='text').get_text()\n",
        "            username = container.find('span', class_='display-name-link').get_text()\n",
        "            review_date = container.find('span', class_='review-date').get_text()\n",
        "\n",
        "            reviews_data.append([username, review_date, review_text])\n",
        "\n",
        "    if reviews_data:\n",
        "        with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "            csv_writer = csv.writer(csvfile)\n",
        "            csv_writer.writerow(['Username', 'Review Date', 'Review Text'])\n",
        "            csv_writer.writerows(reviews_data)\n",
        "\n",
        "        print(f\"{len(reviews_data)} reviews have been successfully scraped and saved to '{output_file}'.\")\n",
        "    else:\n",
        "        print(\"No reviews found on the pages.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    movie_url = 'https://www.imdb.com/title/tt9603212/reviews/?ref_=tt_ql_2'\n",
        "    num_pages_to_scrape = 50\n",
        "    output_csv_file = 'yash_reviews.csv'\n",
        "\n",
        "    scrape_reviews(movie_url, num_pages_to_scrape, output_csv_file)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.read_csv('yash_reviews.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "up3XLhWnnnZL",
        "outputId": "395d3be9-cfa4-467b-ecd5-574cacb95141"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             Username       Review Date  \\\n",
              "0          Paragon240      12 July 2023   \n",
              "1          JackRJosie      15 July 2023   \n",
              "2     ragingbull_2005      18 July 2023   \n",
              "3              imseeg      12 July 2023   \n",
              "4         BA_Harrison      11 July 2023   \n",
              "...               ...               ...   \n",
              "1245      namob-43673      12 July 2023   \n",
              "1246        HalBanksy      10 July 2023   \n",
              "1247         denis888  22 December 2023   \n",
              "1248        Truedutch       9 July 2023   \n",
              "1249          gavinp9       8 July 2023   \n",
              "\n",
              "                                            Review Text  \n",
              "0     Man.... I wish I loved this movie more than I ...  \n",
              "1     Ethan Hunt has left the mere secret agent stat...  \n",
              "2     After the first 30 minutes that promised an in...  \n",
              "3     4 considerations for those with high expectati...  \n",
              "4     Mission Impossible is one of those rare franch...  \n",
              "...                                                 ...  \n",
              "1245  All of the instalments in this franchise are v...  \n",
              "1246  I have just watched the greatest car-chase in ...  \n",
              "1247  I wanted to write a larhe review, making many ...  \n",
              "1248  Straight up Dead Reckoning Part 1 was fine. I ...  \n",
              "1249  'Mission: Impossible - Dead Reckoning Part I (...  \n",
              "\n",
              "[1250 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bc6d8053-b018-4b96-af47-d22312d97910\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Username</th>\n",
              "      <th>Review Date</th>\n",
              "      <th>Review Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Paragon240</td>\n",
              "      <td>12 July 2023</td>\n",
              "      <td>Man.... I wish I loved this movie more than I ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>JackRJosie</td>\n",
              "      <td>15 July 2023</td>\n",
              "      <td>Ethan Hunt has left the mere secret agent stat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ragingbull_2005</td>\n",
              "      <td>18 July 2023</td>\n",
              "      <td>After the first 30 minutes that promised an in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>imseeg</td>\n",
              "      <td>12 July 2023</td>\n",
              "      <td>4 considerations for those with high expectati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>BA_Harrison</td>\n",
              "      <td>11 July 2023</td>\n",
              "      <td>Mission Impossible is one of those rare franch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1245</th>\n",
              "      <td>namob-43673</td>\n",
              "      <td>12 July 2023</td>\n",
              "      <td>All of the instalments in this franchise are v...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1246</th>\n",
              "      <td>HalBanksy</td>\n",
              "      <td>10 July 2023</td>\n",
              "      <td>I have just watched the greatest car-chase in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1247</th>\n",
              "      <td>denis888</td>\n",
              "      <td>22 December 2023</td>\n",
              "      <td>I wanted to write a larhe review, making many ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1248</th>\n",
              "      <td>Truedutch</td>\n",
              "      <td>9 July 2023</td>\n",
              "      <td>Straight up Dead Reckoning Part 1 was fine. I ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1249</th>\n",
              "      <td>gavinp9</td>\n",
              "      <td>8 July 2023</td>\n",
              "      <td>'Mission: Impossible - Dead Reckoning Part I (...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1250 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bc6d8053-b018-4b96-af47-d22312d97910')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bc6d8053-b018-4b96-af47-d22312d97910 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bc6d8053-b018-4b96-af47-d22312d97910');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ea32c10e-7d9d-4bb1-82ea-800b2356b45b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ea32c10e-7d9d-4bb1-82ea-800b2356b45b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ea32c10e-7d9d-4bb1-82ea-800b2356b45b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "'str' object has no attribute 'empty'"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7599cbc1-13e3-42a2-e9f4-d2034ca22a19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "# Download NLTK resources if not already installed\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Read the CSV file containing the raw data\n",
        "df = pd.read_csv('yash_reviews.csv')\n",
        "\n",
        "# Define a function for text cleaning\n",
        "def clean_text(text):\n",
        "    # Remove special characters and punctuation\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Initialize a stemmer and lemmatizer\n",
        "    stemmer = PorterStemmer()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Apply stemming and lemmatization\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n",
        "\n",
        "    # Join the cleaned tokens to form the cleaned text\n",
        "    cleaned_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "# Apply the clean_text function to the 'Review Text' column\n",
        "df['Cleaned Text'] = df['Review Text'].apply(clean_text)\n",
        "\n",
        "# Save the cleaned data to a new CSV file\n",
        "df.to_csv('yash_reviews_cleaned.csv', index=False)\n",
        "\n",
        "print(\"Text data has been cleaned and saved to 'yash_reviews_cleaned.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBgt4ZA1ImNn",
        "outputId": "f76ca4aa-b139-42b8-b232-b123a6fb677d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text data has been cleaned and saved to 'yash_reviews_cleaned.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c59c6d5-678f-42db-c793-a0d458701eba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noun Count: 69050\n",
            "Verb Count: 29050\n",
            "Adjective Count: 27900\n",
            "Adverb Count: 7150\n",
            "\n",
            "Constituency Parsing Tree:\n",
            "man [nsubj] -> wish [ROOT] -> love [compound] -> movi [nsubj] -> do [aux] -> nt [neg] -> get [ccomp] -> wrong [amod] -> solid [amod] -> action [compound] -> movi [compound] -> jawdrop [compound] -> stunt [dobj] -> best [amod] -> seri [amod] -> mission [compound] -> imposs [compound] -> movi [nsubj] -> felt [conj] -> like [prep] -> small [amod] -> step [pobj] -> backward [advmod] -> franchis [det] -> fallout [npadvmod] -> mindblow [amod] -> action [compound] -> sequenc [compound] -> stunt [compound] -> work [dobj] -> along [prep] -> develop [xcomp] -> ethan [compound] -> relationship [dobj] -> ilsa [compound] -> provid [compound] -> closur [compound] -> julia [compound] -> show [compound] -> length [compound] -> ethan [nsubj] -> would [aux] -> go [ccomp] -> protect [advcl] -> closest [amod] -> battl [compound] -> impos [compound] -> villain [nsubj] -> dead [nsubj] -> reckon [ccomp] -> part [nsubj] -> one [nummod] -> stretch [ccomp] -> movi [dobj] -> across [mark] -> two [nummod] -> film [nmod] -> seemingli [compound] -> showcas [nmod] -> action [compound] -> spectacl [appos] -> action [compound] -> spectacl [compound] -> sacrif [compound] -> charact [nsubj] -> develop [advcl] -> charact [nmod] -> grown [amod] -> love [compound] -> decad [compound] -> film [nsubj] -> felt [conj] -> sidelin [compound] -> ignor [compound] -> wast [compound] -> hayley [nsubj] -> atwel [ccomp] -> new [amod] -> charact [compound] -> chew [compound] -> screen [compound] -> time [compound] -> fantast [nsubj] -> want [ccomp] -> see [xcomp] -> origin [compound] -> team [compound] -> new [amod] -> villain [compound] -> inconsist [compound] -> abil [dobj] -> confus [npadvmod] -> intimid [compound] -> import [compound] -> emot [compound] -> moment [nsubj] -> did [aux] -> nt [neg] -> feel [conj] -> weight [compound] -> definit [compound] -> part [nsubj] -> two [nsubj] -> might [aux] -> tie [ccomp] -> everyth [compound] -> togeth [dobj] -> make [advcl] -> enjoy [nsubj] -> part [dobj] -> one [nummod] -> retrospect [amod] -> unfortun [amod] -> left [nsubj] -> want [ccomp] -> one [dobj] -> \n",
            "\n",
            "Dependency Parsing Tree:\n",
            "man [wish] -> wish [wish] -> love [movi] -> movi [get] -> do [get] -> nt [get] -> get [wish] -> wrong [stunt] -> solid [movi] -> action [movi] -> movi [stunt] -> jawdrop [stunt] -> stunt [get] -> best [movi] -> seri [movi] -> mission [imposs] -> imposs [movi] -> movi [felt] -> felt [wish] -> like [felt] -> small [step] -> step [like] -> backward [step] -> franchis [work] -> fallout [mindblow] -> mindblow [action] -> action [work] -> sequenc [stunt] -> stunt [work] -> work [felt] -> along [work] -> develop [felt] -> ethan [relationship] -> relationship [develop] -> ilsa [ethan] -> provid [closur] -> closur [show] -> julia [show] -> show [length] -> length [ethan] -> ethan [go] -> would [go] -> go [felt] -> protect [go] -> closest [villain] -> battl [impos] -> impos [villain] -> villain [reckon] -> dead [reckon] -> reckon [protect] -> part [stretch] -> one [part] -> stretch [reckon] -> movi [stretch] -> across [develop] -> two [showcas] -> film [showcas] -> seemingli [showcas] -> showcas [charact] -> action [spectacl] -> spectacl [showcas] -> action [charact] -> spectacl [sacrif] -> sacrif [charact] -> charact [develop] -> develop [reckon] -> charact [love] -> grown [love] -> love [film] -> decad [film] -> film [felt] -> felt [wish] -> sidelin [ignor] -> ignor [hayley] -> wast [hayley] -> hayley [atwel] -> atwel [felt] -> new [chew] -> charact [chew] -> chew [time] -> screen [time] -> time [fantast] -> fantast [want] -> want [atwel] -> see [want] -> origin [team] -> team [abil] -> new [abil] -> villain [inconsist] -> inconsist [abil] -> abil [see] -> confus [see] -> intimid [emot] -> import [emot] -> emot [moment] -> moment [feel] -> did [feel] -> nt [feel] -> feel [wish] -> weight [definit] -> definit [part] -> part [tie] -> two [tie] -> might [tie] -> tie [feel] -> everyth [togeth] -> togeth [tie] -> make [wish] -> enjoy [retrospect] -> part [enjoy] -> one [part] -> retrospect [left] -> unfortun [left] -> left [want] -> want [make] -> one [want] -> \n",
            "\n",
            "Named Entity Counts:\n",
            "PERSON: 4550\n",
            "ORG: 1200\n",
            "LOC: 100\n",
            "PRODUCT: 0\n",
            "DATE: 450\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Reads the cleaned text data\n",
        "df = pd.read_csv('yash_reviews_cleaned.csv')\n",
        "\n",
        "# (1)(POS) Tagging\n",
        "noun_count = 0\n",
        "verb_count = 0\n",
        "adj_count = 0\n",
        "adv_count = 0\n",
        "\n",
        "for text in df['Cleaned Text']:\n",
        "    doc = nlp(text)\n",
        "    for token in doc:\n",
        "        if token.pos_ == \"NOUN\":\n",
        "            noun_count += 1\n",
        "        elif token.pos_ == \"VERB\":\n",
        "            verb_count += 1\n",
        "        elif token.pos_ == \"ADJ\":\n",
        "            adj_count += 1\n",
        "        elif token.pos_ == \"ADV\":\n",
        "            adv_count += 1\n",
        "\n",
        "print(f\"Noun Count: {noun_count}\")\n",
        "print(f\"Verb Count: {verb_count}\")\n",
        "print(f\"Adjective Count: {adj_count}\")\n",
        "print(f\"Adverb Count: {adv_count}\")\n",
        "\n",
        "# Constituency Parsing and Dependency Parsing (using one sentence as an example)\n",
        "sample_text = df['Cleaned Text'].iloc[0]  # Take the first sentence as an example\n",
        "\n",
        "# Constituency Parsing Tree\n",
        "sample_doc = nlp(sample_text)\n",
        "print(\"\\nConstituency Parsing Tree:\")\n",
        "for token in sample_doc:\n",
        "    print(f\"{token.text} [{token.dep_}]\", end=\" -> \")\n",
        "print()\n",
        "\n",
        "# Dependency Parsing Tree\n",
        "print(\"\\nDependency Parsing Tree:\")\n",
        "for token in sample_doc:\n",
        "    print(f\"{token.text} [{token.head.text}]\", end=\" -> \")\n",
        "print()\n",
        "\n",
        "# Named Entity Recognition\n",
        "entities = {\n",
        "    \"PERSON\": 0,\n",
        "    \"ORG\": 0,\n",
        "    \"LOC\": 0,\n",
        "    \"PRODUCT\": 0,\n",
        "    \"DATE\": 0\n",
        "}\n",
        "\n",
        "for text in df['Cleaned Text']:\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in entities:\n",
        "            entities[ent.label_] += 1\n",
        "\n",
        "print(\"\\nNamed Entity Counts:\")\n",
        "for entity, count in entities.items():\n",
        "    print(f\"{entity}: {count}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation about constituency parsing tree and dependency parsing tree with example**\n",
        "\n",
        "Constituency parsing represents the sentence \"Man, I wish I loved this movie more than I did\" as a hierarchical tree structure, breaking it into phrases such as noun phrases (NP) and verb phrases (VP), as seen in the example tree provided earlier. In contrast, dependency parsing illustrates the grammatical relationships between words, showcasing dependencies like the subject-verb relationship between \"Man\" and \"wish\" or the object-verb relationship between \"movie\" and \"loved,\" as demonstrated in the dependency tree example."
      ],
      "metadata": {
        "id": "T5_s8_BLub6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your response below\n",
        "\n",
        "print(\"In this assignment, the code for parsing is a bit challenging, but aside from that, I have learned many new methods to scrape and clean the data. Overall, I am acquiring a wealth of knowledge in coding through these assignments\")"
      ],
      "metadata": {
        "id": "gZ8dwgGKZoX-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a10ce5e-a34b-4ec9-c0c7-8d7afcdcf67f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In this assignment, the code for parsing is a bit challenging, but aside from that, I have learned many new methods to scrape and clean the data. Overall, I am acquiring a wealth of knowledge in coding through these assignments\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}